{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shubhangsrivastav/image-caption-/blob/master/Image_Caption_Generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTdBNUeB_e2S",
        "outputId": "9b1111ae-c1b1-4256-85ee-8dc892556870"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==2.7\n",
            "  Downloading https://us-python.pkg.dev/colab-wheels/public/tensorflow/tensorflow-2.7.0%2Bzzzcolab20220506150900-cp37-cp37m-linux_x86_64.whl (665.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 665.5 MB 24 kB/s \n",
            "\u001b[?25hCollecting flatbuffers<3.0,>=1.12\n",
            "  Downloading flatbuffers-2.0.7-py2.py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7) (1.14.1)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7) (14.0.6)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7) (4.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7) (3.17.3)\n",
            "Collecting keras<2.8,>=2.7.0rc0\n",
            "  Downloading keras-2.7.0-py2.py3-none-any.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 56.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7) (2.0.1)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7) (1.6.3)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7) (1.21.6)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7) (2.8.0)\n",
            "Collecting tensorflow-estimator<2.8,~=2.7.0rc0\n",
            "  Downloading tensorflow_estimator-2.7.0-py2.py3-none-any.whl (463 kB)\n",
            "\u001b[K     |████████████████████████████████| 463 kB 54.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7) (1.49.1)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7) (1.2.0)\n",
            "Collecting gast<0.5.0,>=0.2.1\n",
            "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7) (0.27.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7) (1.1.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7) (0.37.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.7) (1.15.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow==2.7) (1.5.2)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7) (1.8.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7) (1.35.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow==2.7) (3.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7) (4.9)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.7) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.7) (5.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow==2.7) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow==2.7) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow==2.7) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow==2.7) (3.2.1)\n",
            "Installing collected packages: tensorflow-estimator, keras, gast, flatbuffers, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.8.0\n",
            "    Uninstalling keras-2.8.0:\n",
            "      Successfully uninstalled keras-2.8.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.3\n",
            "    Uninstalling gast-0.5.3:\n",
            "      Successfully uninstalled gast-0.5.3\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 22.9.24\n",
            "    Uninstalling flatbuffers-22.9.24:\n",
            "      Successfully uninstalled flatbuffers-22.9.24\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.2+zzzcolab20220929150707\n",
            "    Uninstalling tensorflow-2.8.2+zzzcolab20220929150707:\n",
            "      Successfully uninstalled tensorflow-2.8.2+zzzcolab20220929150707\n",
            "Successfully installed flatbuffers-2.0.7 gast-0.4.0 keras-2.7.0 tensorflow-2.7.0+zzzcolab20220506150900 tensorflow-estimator-2.7.0\n"
          ]
        }
      ],
      "source": [
        " !pip install tensorflow==2.7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6N2Edqij6CWw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "from keras.applications.imagenet_utils import preprocess_input\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.utils import to_categorical, plot_model\n",
        "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, add\n",
        "from keras.applications.resnet import ResNet50\n",
        "from tensorflow.python.keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.applications.resnet_v2 import ResNet101V2\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/',force_remount=True)"
      ],
      "metadata": {
        "id": "HI2cJTMEdBN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jqk0drANDQg2"
      },
      "outputs": [],
      "source": [
        "BASE_DIR = '/content/drive/MyDrive/archive (5)'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRXVghNdC83q"
      },
      "outputs": [],
      "source": [
        "# load resnet 50 model\n",
        "model =ResNet50()\n",
        "# restructure the model\n",
        "model = Model(inputs=model.inputs, outputs=model.layers[-2].output)# here we are removing fully connected layer as it is not required for image extraction.\n",
        "# summarize\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CqoUe94Eqgln"
      },
      "outputs": [],
      "source": [
        "# extract features from image\n",
        "features = {}\n",
        "directory = os.path.join(BASE_DIR, 'Images')\n",
        "\n",
        "for img_name in tqdm(os.listdir(directory)):\n",
        "    # load the image from file\n",
        "    img_path = directory + '/' + img_name\n",
        "    image = load_img(img_path, target_size=(224, 224))\n",
        "    # convert image pixels to numpy array\n",
        "    image = img_to_array(image)\n",
        "    # reshape data for model as it is a rgb image so 3 parameter\n",
        "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "    # preprocess image for resnet 50\n",
        "    image = preprocess_input(image)\n",
        "    # extract features\n",
        "    feature = model.predict(image, verbose=0)\n",
        "    # get image ID\n",
        "    image_id = img_name.split('.')[0]\n",
        "    # store feature\n",
        "    features[image_id] = feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lojSnVkerOKi"
      },
      "outputs": [],
      "source": [
        "with open(os.path.join(BASE_DIR, 'captions.txt'), 'r') as f:\n",
        "    next(f)\n",
        "    captions_doc = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhaHr2YesH9c"
      },
      "outputs": [],
      "source": [
        "# create mapping of image to captions\n",
        "mapping = {}\n",
        "# process lines\n",
        "for line in tqdm(captions_doc.split('\\n')):\n",
        "    # split the line by comma(,)\n",
        "    tokens = line.split(',')\n",
        "    if len(line) < 2:\n",
        "        continue\n",
        "    image_id, caption = tokens[0], tokens[1:]\n",
        "    # remove extension from image ID\n",
        "    image_id = image_id.split('.')[0]\n",
        "    # convert caption list to string\n",
        "    caption = \" \".join(caption)\n",
        "    # create list if needed\n",
        "    if image_id not in mapping:\n",
        "        mapping[image_id] = []\n",
        "    # store the caption\n",
        "    mapping[image_id].append(caption)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pickle.dump(features,open('features.pkl','wb'))"
      ],
      "metadata": {
        "id": "W_0G92bh0y_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ocdfqXpsMzG"
      },
      "outputs": [],
      "source": [
        "len(mapping)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5qRskRBsVVU"
      },
      "outputs": [],
      "source": [
        "def clean(mapping):\n",
        "    for key, captions in mapping.items():\n",
        "        for i in range(len(captions)):\n",
        "            # take one caption at a time\n",
        "            caption = captions[i]\n",
        "            # preprocessing steps\n",
        "            # convert to lowercase\n",
        "            caption = caption.lower()\n",
        "            # delete digits, special chars, etc., \n",
        "            caption = caption.replace('[^A-Za-z]', '')\n",
        "            # delete additional spaces\n",
        "            caption = caption.replace('\\s+', ' ')# for multiple spaces.\n",
        "            # add start and end tags to the caption\n",
        "            caption = 'startseq ' + \" \".join([word for word in caption.split() if len(word)>1]) + ' endseq'#removing single letters in the sentences like 'a' etc.\n",
        "            captions[i] = caption"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9O3Av1esXKj"
      },
      "outputs": [],
      "source": [
        "#preprocess the text\n",
        "clean(mapping)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "flTXZXaIsdfL"
      },
      "outputs": [],
      "source": [
        "#after preprocess of text\n",
        "mapping['1000268201_693b08cb0e']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pickle.dump(mapping,open('mapping.pkl','wb'))"
      ],
      "metadata": {
        "id": "wl8r48kv1MOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VK5xO1ngsfOz"
      },
      "outputs": [],
      "source": [
        "# calculating number of captions(words used in description of image) for each image \n",
        "all_captions=[]\n",
        "for key in mapping:\n",
        "    for caption in mapping[key]:\n",
        "        all_captions.append(caption)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jVytZqceskJC"
      },
      "outputs": [],
      "source": [
        "len(all_captions)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pickle.dump(all_captions,open('all_captions.pkl','wb'))"
      ],
      "metadata": {
        "id": "a0YgndiYNiht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xwhpj8oIsqCv"
      },
      "outputs": [],
      "source": [
        "#Tokenising text\n",
        "tokenizer=Tokenizer()\n",
        "tokenizer.fit_on_texts(all_captions)\n",
        "vocab_size=len(tokenizer.word_index)+1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pickle.dump(tokenizer,open('tokenizer.pkl','wb'))"
      ],
      "metadata": {
        "id": "5LZaFZS21Wqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pickle.dump(vocab_size,open('vocab_size.pkl','wb'))"
      ],
      "metadata": {
        "id": "2rPRmqWM1gss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9n4C5UTLssSs"
      },
      "outputs": [],
      "source": [
        "vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-1g9tWUsxeF"
      },
      "outputs": [],
      "source": [
        "#get maximum length of the caption available for padding purpose\n",
        "max_length=max(len(caption.split()) for caption in all_captions)\n",
        "max_length"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pickle.dump(max_length,open('max_length.pkl','wb'))"
      ],
      "metadata": {
        "id": "ZojbTPNs1rT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYqzkqC4szuM"
      },
      "outputs": [],
      "source": [
        "image_ids = list(mapping.keys())\n",
        "split = int(len(image_ids) * 0.90)\n",
        "train = image_ids[:split]\n",
        "test = image_ids[split:]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pickle.dump(image_ids,open('image_ids.pkl','wb'))"
      ],
      "metadata": {
        "id": "AygnG_Fv1yRh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pickle.dump(train,open('train.pkl','wb'))"
      ],
      "metadata": {
        "id": "A4-A2Fmn18DW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bNeAiCis8Tt"
      },
      "outputs": [],
      "source": [
        "# create data generator to get data in batch (avoids session crash)\n",
        "def data_generator(data_keys, mapping, features, tokenizer, max_length, vocab_size, batch_size):\n",
        "    # loop over images\n",
        "    X1, X2, y = list(), list(), list()\n",
        "    n = 0\n",
        "    while 1:\n",
        "        for key in data_keys:\n",
        "            n += 1\n",
        "            captions = mapping[key]\n",
        "            # process each caption\n",
        "            for caption in captions:\n",
        "                # encode the sequence\n",
        "                seq = tokenizer.texts_to_sequences([caption])[0]\n",
        "                # split the sequence into X, y pairs\n",
        "                for i in range(1, len(seq)):\n",
        "                    # split into input and output pairs\n",
        "                    in_seq, out_seq = seq[:i], seq[i]\n",
        "                    # pad input sequence\n",
        "                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "                    # encode output sequence\n",
        "                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "                    \n",
        "                    # store the sequences\n",
        "                    X1.append(features[key][0])\n",
        "                    X2.append(in_seq)\n",
        "                    y.append(out_seq)\n",
        "            if n == batch_size:\n",
        "                X1, X2, y = np.array(X1), np.array(X2), np.array(y)\n",
        "                yield [X1, X2], y\n",
        "                X1, X2, y = list(), list(), list()\n",
        "                n = 0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pickle.dump(data_generator,open('data_generator.pkl','wb'))"
      ],
      "metadata": {
        "id": "KeUI4BFhfy2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IPZi4920tDTa"
      },
      "outputs": [],
      "source": [
        "# encoder model\n",
        "# image feature layers\n",
        "inputs1 = Input(shape=(2048,))\n",
        "fe1 = Dropout(0.4)(inputs1)\n",
        "fe2 = Dense(256, activation='relu')(fe1)\n",
        "# sequence feature layers\n",
        "inputs2 = Input(shape=(max_length,))\n",
        "se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
        "se2 = Dropout(0.4)(se1)\n",
        "se3 = LSTM(256)(se2)\n",
        "\n",
        "# decoder model\n",
        "decoder1 = add([fe2, se3])\n",
        "decoder2 = Dense(256, activation='relu')(decoder1)\n",
        "outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
        "\n",
        "model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "# plot the model\n",
        "plot_model(model, show_shapes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GtTP1rEetH4-"
      },
      "outputs": [],
      "source": [
        "# train the model\n",
        "epochs = 40\n",
        "batch_size = 32\n",
        "steps = len(train) // batch_size\n",
        "\n",
        "for i in range(epochs):\n",
        "    # create data generator\n",
        "    generator = data_generator(train, mapping, features, tokenizer, max_length, vocab_size, batch_size)\n",
        "    # fit for one epoch\n",
        "    model.fit(generator, epochs=1, steps_per_epoch=steps, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBQkqDdgWr63"
      },
      "source": [
        "##Generate Captions for the image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FkUrhv7UWgAq"
      },
      "outputs": [],
      "source": [
        "#save the model\n",
        "pickle.dump(model,open('model.pkl','wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1GAuPpmPP2y"
      },
      "outputs": [],
      "source": [
        "def idx_to_word(integer, tokenizer):\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == integer:\n",
        "            return word\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pickle.dump(idx_to_word,open('idx_to_word.pkl','wb'))"
      ],
      "metadata": {
        "id": "TladvvU4F9P9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0aC-6HC7PUlJ"
      },
      "outputs": [],
      "source": [
        "# generate caption for an image\n",
        "def predict_caption(model, image, tokenizer, max_length):\n",
        "    # add start tag for generation process\n",
        "    in_text = 'startseq'\n",
        "    # iterate over the max length of sequence\n",
        "    for i in range(max_length):\n",
        "        # encode input sequence\n",
        "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        # pad the sequence\n",
        "        sequence = pad_sequences([sequence], max_length)\n",
        "        # predict next word\n",
        "        yhat = model.predict([image, sequence], verbose=0)\n",
        "        # get index with high probability\n",
        "        yhat = np.argmax(yhat)\n",
        "        # convert index to word\n",
        "        word = idx_to_word(yhat, tokenizer)\n",
        "        # stop if word not found\n",
        "        if word is None:\n",
        "            break\n",
        "        # append word as input for generating next word\n",
        "        in_text += \" \" + word\n",
        "        # stop if we reach end tag\n",
        "        if word == 'endseq':\n",
        "            break\n",
        "      \n",
        "    return in_text\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pickle.dump(predict_caption,open('predict_caption.pkl','wb'))"
      ],
      "metadata": {
        "id": "xRojQMTV2u3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfOsaGDzW_8z"
      },
      "source": [
        "##Validating Model¶"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBrlnwmPPYnj"
      },
      "outputs": [],
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "# validate with test data\n",
        "actual, predicted = list(), list()\n",
        "\n",
        "for key in tqdm(test):\n",
        "    # get actual caption\n",
        "    captions = mapping[key]\n",
        "    # predict the caption for image\n",
        "    y_pred = predict_caption(model, features[key], tokenizer, max_length) \n",
        "    # split into words\n",
        "    actual_captions = [caption.split() for caption in captions]\n",
        "    y_pred = y_pred.split()\n",
        "    # append to the list\n",
        "    actual.append(actual_captions)\n",
        "    predicted.append(y_pred)\n",
        "    \n",
        "# calcuate BLEU score\n",
        "print(\"BLEU-1: %f\" % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "print(\"BLEU-2: %f\" % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hzGbbUIiiQ3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQwOA-_UiRSf"
      },
      "outputs": [],
      "source": [
        "               \n",
        "                        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNtdlgXjPdZC"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "def generate_caption(image_name):\n",
        "    #Load the image\n",
        "\n",
        "    image_id =image_name.split('.')[0]\n",
        "    img_path=os.path.join(BASE_DIR,\"Images\",image_name)\n",
        "    image=Image.open(img_path)\n",
        "    captions=mapping[image_id]\n",
        "    print('Actual')\n",
        "    for caption in captions:\n",
        "        print(caption)\n",
        "        #predict the caption\n",
        "    y_pred=predict_caption(model,features[image_id],tokenizer,max_length)\n",
        "    print('Predicted')\n",
        "    print(y_pred)\n",
        "    plt.imshow(image)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o480ljjVPikA"
      },
      "outputs": [],
      "source": [
        "generate_caption('1028205764_7e8df9a2ea.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XZAz1EvRPlsc"
      },
      "outputs": [],
      "source": [
        "generate_caption('1048710776_bb5b0a5c7c.jpg')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ih1_tV0RPqaW"
      },
      "outputs": [],
      "source": [
        "generate_caption('1075867198_27ca2e7efe.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chcpjiUyOQjF"
      },
      "outputs": [],
      "source": [
        "generate_caption('1248940539_46d33ed487.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GH0g9xoMOQZb"
      },
      "outputs": [],
      "source": [
        "generate_caption('1350948838_fdebe4ff65.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8AXBQTMOQUB"
      },
      "outputs": [],
      "source": [
        "generate_caption('1449625950_fc9a8d02d9.jpg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjmg8CkUOQMv"
      },
      "outputs": [],
      "source": [
        "generate_caption('1562392511_522a26063b.jpg')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOku0Ck2J+qnorQuPhKxBpq",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}